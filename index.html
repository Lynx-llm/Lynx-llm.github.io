<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="Lynx">
  <meta name="keywords" content="Lynx vision-language">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Lynx</title>

  <meta name="google-site-verification" content="6lbYN1vX7A4sD8SrVniq84UEKyEUSBgxeP7d3FjuuK0" />

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="icon" href="./static/images/logo_plus.png">
  <link rel="stylesheet" href="./static/css/index.css">

  <link rel="shortcut icon" href="path/to/favicon.ico" type="image/x-icon">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  </head>

  <style>

    #main{
        position: relative;;
        width: 1200px;
    }

    .box{
        float: left;
        padding: 15px 0 0 15px;
        /*background-color: red;*/
    }

    .pic{
        width: 500px;
        padding: 10px;
        border: 1px solid #ccc;
        border-radius: 5px;
        background-color: #fff;
    }

    .pic img{
        width: 500px;
    }

    .div-height{height:200px}

  </style>



  <body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <img id="" width="12%" src="static/images/logo_plus.png">
          <h2 class="title is-2 publication-title">What Matters in Training a GPT4-Style Language Model with Multimodal Inputs?</h2>
          <div class="is-size-5">
            <span class="author-block">
              Yan Zeng<sup>*</sup></a>,
            </span>
            <span class="author-block">
              Hanbo Zhang<sup>*</sup></a>,
            </span>
            <span class="author-block">
              Jiani Zheng<sup>*</sup></a>,
            </span>
            <span class="author-block">
              Jiangnan Xia</a>,
            </span>
            <span class="author-block">
              Guoqiang Wei</a>,
            </span>
          </div>

          <div class="is-size-5">
            <span class="author-block">
              Yang Wei</a>,
            </span>
            <span class="author-block">
              Yuchen Zhang</a>,
            </span>
            <span class="author-block">
              Tao Kong</a>
            </span>
          </div>
          <br>
          <div class="is-size-5 publication-authors">
            <span class="author-block">ByteDance Research &nbsp;&nbsp;</span>
            <span class="author-block"><sup>*</sup>Equal Contribution </span>
          </div>

<!--          <div class="is-size-5 publication-authors">-->
<!--            <span class="author-block"><sup>*</sup>Equal Contribution </span>-->
<!--          </div>-->

          <br>


          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="https://arxiv.org/abs/2307.02469" target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>

             <span class="link-block">
                <a href="https://github.com/bytedance/lynx-llm" target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>

<!--              <span class="link-block">-->
<!--                <a href="https://huggingface.co/" target="_blank"-->
<!--                   class="external-link button is-normal is-rounded is-dark">-->
<!--                  <span class="icon">-->
<!--                    <i class="fa fa-laugh"></i>-->
<!--                  </span>-->
<!--                  <span>Model</span>-->
<!--                  </a>-->
<!--              </span>-->

            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<link rel="stylesheet" type="text/css" href="js/simple_style.css" />
<script type="text/javascript" src="js/simple_swiper.js"></script>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Recent advancements in Large Language Models (LLMs) such as GPT4 have dis-
          played exceptional multi-modal capabilities in following open-ended instructions
          given images. However, the performance of these models heavily relies on design
          choices such as network structures, training data, and training strategies, and these
          choices have not been extensively discussed in the literature, making it difficult to
          quantify progress in this field. To address this issue, this paper presents a systematic
          and comprehensive study, quantitatively and qualitatively, on training such models.
          We implement over 20 variants with controlled settings. Concretely, for network
          structures, we compare different LLM backbones and model designs. For training
          data, we investigate the impact of data and sampling strategies. For instructions,
          we explore the influence of diversified prompts on the instruction-following ability
          of the trained models. For benchmarks, we contribute the first, to our best knowl-
          edge, comprehensive evaluation set including both image and video tasks through
            crowd-sourcing. Based on our findings, we present <b>Lynx</b>, which performs the most
          accurate multi-modal understanding while keeping the best multi-modal generation
          ability compared to existing open-sourced GPT4-style models.
          </p>
        </div>
      </div>
    </div>
    <br>
    <br>

    <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths">
        <h2 class="title is-3">Model</h2>
        <div class="content has-text-justified">
          <p>
            Our model takes simultaneously vision and language as inputs to generate text responses following the input
            instructions. Concretely, vision inputs are first processed by a vision encoder to get a sequence of vision
            tokens. After that, vision tokens are fused with instruction tokens for multi-modal tasks. In our model,
            we directly concatenate the projected vision tokens and instruction tokens as the input of LLMs, which can
            then be processed by the decoder-only LLMs naturally. We call this structure "prefix-finetuning" (PT)          </p>
        </div>
        <img id="model" width="100%" src="static/images/lynx.png">
        <h3 class="has-text-centered">
          <p style="font-family:Times New Roman"><b>The architecture of Lynx.</b></p>
        </h3>

      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="columns is-centered has-text-centered">
      <h2 class="title is-3">Results</h2>
  </div>
  <div class="container is-max-desktop">
    <div class="columns is-centered">
    <div class="column">
      <div class="content">
        <img id="model" width="100%" src="static/images/result_1.png">
      </div>
    </div>
    </div>
    <div class="columns is-centered">
    <div class="column">
      <div class="content">
        <img id="model" width="90%" src="static/images/result_all.png">
        <div class="content has-text-justified">
          <p>We first compare existing open-sourced multi-modal LLMs and quantitative evaluation results on the Open-VQA
            image benchmark, and then (a) benchmark them on videos; (b) benchmark them on MME dataset; (c) show human
            evaluation results, and (d) conduct an in-depth ablation study to investigate the impact of different
            components.</p>
        </div>
      </div>
    </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="columns is-centered has-text-centered">
      <h2 class="title is-3">Demos</h2>
  </div>

  <div class="container is-max-desktop">
    <div class="columns is-centered">
    <div class="column">
      <div class="content">
        <img src="static/images/vqa_exp_cases.png">
      </div>
    </div>
    </div>
  </div>

  <div class="container is-max-desktop">
    <div class="columns is-centered">
    <div class="column">
      <div class="content">
        <img src="static/images/mplug_exp_cases.png">
      </div>
    </div>
    </div>
  </div>

  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
    <div class="column">
      <div class="content">
        <img width="100%" src="static/images/more_video_vqa_cases.png">
      </div>
    </div>
    </div>
  </div>
</section>
</section>

<section class="section" id="BibTeX">
<div class="container is-max-desktop content">
  <h2 class="title">BibTeX</h2>
  <pre><code>

@article{zeng2023matters,
  title={What Matters in Training a GPT4-Style Language Model with Multimodal Inputs?},
  author={Zeng, Yan and Zhang, Hanbo and Zheng, Jiani and Xia, Jiangnan and Wei, Guoqiang and Wei, Yang and Zhang, Yuchen and Kong, Tao},
  journal={arXiv preprint arXiv:2307.02469},
  year={2023}
}
</code></pre>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
        <div class="content">
          <p>
            The website template was adapted from <a
              href="https://github.com/nerfies/nerfies.github.io">nerfies</a>.
          </p>
        </div>
      </div>
  </div>
</footer>
</body>

</html>
