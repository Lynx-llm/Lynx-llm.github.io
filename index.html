<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="Lynx">
  <meta name="keywords" content="GPT-4, open-source, vision-language">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Lynx</title>

  <meta name="google-site-verification" content="6lbYN1vX7A4sD8SrVniq84UEKyEUSBgxeP7d3FjuuK0" />

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="icon" href="./static/images/logo.png">
  <link rel="stylesheet" href="./static/css/index.css">

  <link rel="shortcut icon" href="path/to/favicon.ico" type="image/x-icon">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  </head>

  <style>

    #main{
        position: relative;;
        width: 1200px;
    }

    .box{
        float: left;
        padding: 15px 0 0 15px;
        /*background-color: red;*/
    }

    .pic{
        width: 500px;
        padding: 10px;
        border: 1px solid #ccc;
        border-radius: 5px;
        background-color: #fff;
    }

    .pic img{
        width: 500px;
    }

    .div-height{height:200px}

  </style>



  <body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Lynx</h1>
          <h2 class="title is-2 publication-title">What Matters in Training a GPT4-Style Language Model with Multimodal Inputs?</h2>
          <div class="is-size-5">
            <span class="author-block">
                <a href="" style="color:#008AD7;font-weight:normal;">Yan Zeng<sup>*</sup></a>,
            </span>
            <span class="author-block">
              <a href="" style="color:#008AD7;font-weight:normal;">Hanbo Zhang<sup>*</sup></a>,
            </span>
            <span class="author-block">
              <a href="" style="color:#008AD7;font-weight:normal;">Jiani Zheng<sup>*</sup></a>,
            </span>
            <span class="author-block">
              <a href="" style="color:#008AD7;font-weight:normal;">Jiangnan Xia</a>,
            </span>
            <span class="author-block">
              <a href="" style="color:#008AD7;font-weight:normal;">Guoqiang Wei</a>,
            </span>
          </div>

          <div class="is-size-5">
            <span class="author-block">
              <a href="" style="color:#008AD7;font-weight:normal;">Yang Wei</a>,
            </span>
            <span class="author-block">
              <a href="" style="color:#008AD7;font-weight:normal;">Yuchen Zhang</a>,
            </span>
            <span class="author-block">
              <a href="" style="color:#008AD7;font-weight:normal;">Tao Kong</a>
            </span>
          </div>
          <br>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><b style="color:#008AD7; font-weight:normal">&#x25B6 </b> ByteDance Research </span>
            <!-- <span class="author-block"><b style="color:#F2A900; font-weight:normal">&#x25B6 </b>UCLA; </span> -->
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>*</sup>Equal Contribution </span>
          </div>

          <br>


          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="https://arxiv.org/abs/" target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>

              <span class="link-block">
                <a href="https://github.com/" target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>

              <span class="link-block">
                <a href="https://huggingface.co/" target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fa fa-laugh"></i>
                  </span>
                  <span>Model</span>
                  </a>
              </span>

            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<link rel="stylesheet" type="text/css" href="js/simple_style.css" />
<script type="text/javascript" src="js/simple_swiper.js"></script>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Recent advancements in Large Language Models (LLMs) such as GPT4 have dis-
          played exceptional multi-modal capabilities in following open-ended instructions
          given images. However, the performance of these models heavily relies on design
          choices such as network structures, training data, and training strategies, and these
          choices have not been extensively discussed in the literature, making it difficult to
          quantify progress in this field. To address this issue, this paper presents a systematic
          and comprehensive study, quantitatively and qualitatively, on training such models.
          We implement over 20 variants with controlled settings. Concretely, for network
          structures, we compare different LLM backbones and model designs. For training
          data, we investigate the impact of data and sampling strategies. For instructions,
          we explore the influence of diversified prompts on the instruction-following ability
          of the trained models. For benchmarks, we contribute the first, to our best knowl-
          edge, comprehensive evaluation set including both image and video tasks through
            crowd-sourcing. Based on our findings, we present <b>Lynx</b>, which performs the most
          accurate multi-modal understanding while keeping the best multi-modal generation
          ability compared to existing open-sourced GPT4-style models.
          </p>
        </div>
      </div>
    </div>
    <br>
    <br>

    <br>
    <br>
    <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths">
        <h2 class="title is-3">Model</h2>
        <div class="content has-text-justified">
          <p>
            Our model is based on prefix-tuning architecture: the vision tokens are directly concatenated with the text tokens to generate outputs auto-regressively.
          </p>
        </div>
        <img id="model" width="100%" src="static/images/lynx.png">
        <h3 class="has-text-centered">
          <p style="font-family:Times New Roman"><b>The architecture of Lynx.</b></p>
        </h3>
        <br>
        <br>

      </div>
    </div>
    <br>
    <br>
  </div>
</section>

<section class="section">
  <div class="columns is-centered has-text-centered">
    <div class="column is-six-fifths">
      <h2 class="title is-3">Results</h2>
    </div>
  </div>

  <div class="container is-max-desktop"></div>
</section>


<script src="js/Underscore-min.js"></script>
<script src="js/index.js"></script>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">

    <div class="column">
      <div class="content">
        <img id="model" width="100%" src="static/images/result_1.png">
        <h5>
          <p style="font-family:Times New Roman">Comparison of existing open-sourced multi-modal LLMs and quantitative evaluation results (accuracy) on our Open-VQA image test set.</p>
        </h5>
      </div>
    </div>

    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">

    <div class="column">
      <div class="content">
        <img id="model" width="40%" src="static/images/result_2.png">
        <h5>
          <p style="font-family:Times New Roman">Comparison of existing open-sourced multi-modal LLMs on the Open-VQA video benchmark.</p>
        </h5>
      </div>
    </div>

    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered ">

    <div class="column">
      <div class="content">
        <img id="model" width="40%" src="static/images/result_3.png">
        <h5>
          <p style="font-family:Times New Roman">Comparison of human-evaluation performance on OwlEval. Scores are averaged over the number of questions.</p>
        </h5>
      </div>
    </div>

    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">

    <div class="column">
      <div class="content">
        <img id="model" width="40%" src="static/images/mme.png">
        <h5>
          <p style="font-family:Times New Roman">Comparison on MME benchmark.</p>
        </h5>
      </div>
    </div>

    </div>
  </div>
</section>


<section class="section">
  <div class="columns is-centered has-text-centered">
    <div class="column is-six-fifths">
      <h2 class="title is-3">Cases</h2>
    </div>
  </div>

  <div class="container is-max-desktop"></div>
</section>

<!--<section class="section">-->
<!--  <div class="container is-max-desktop">-->
<!--    <div class="columns is-centered">-->

<!--    <div class="column">-->
<!--      <div class="content">-->
<!--        <img src="static/images/bench_cases.png">-->
<!--        <h5>-->
<!--          <p style="font-family:Times New Roman"> Examples of our test set. (a) Open-VQA benchmark to validate the accuracy of visual understanding; (b) OwlEval to evaluate the quality of language generation.</p>-->
<!--        </h5>-->
<!--      </div>-->
<!--    </div>-->

<!--    </div>-->
<!--  </div>-->
<!--</section>-->

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">

    <div class="column">
      <div class="content">
        <img src="static/images/vqa_exp_cases.png">
        <h5>
          <p style="font-family:Times New Roman">Qualitative results on our Open-VQA benchmark of different models. We choose Instruct-BLIP and mPLUG-Owl because they perform best on the Open-VQA benchmark and OwlEval benchmark in all baseline algorithms.</p>
        </h5>
      </div>
    </div>

    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">

    <div class="column">
      <div class="content">
        <img src="static/images/mplug_exp_cases.png">
        <h5>
          <p style="font-family:Times New Roman"> Qualitative results on OwlEval benchmark of different models. We choose InstructBLIP
            and mPLUG-Owl because they perform best on the Open-VQA benchmark and OwlEval benchmark
            in all baseline algorithms.
          </p>
        </h5>
      </div>
    </div>

    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">

    <div class="column">
      <div class="content">
        <img src="static/images/more_image_vqa_cases.png">
        <h5>
          <p style="font-family:Times New Roman">More cases on our Open-VQA image benchmark.
          </p>
        </h5>
      </div>
    </div>

    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">

    <div class="column">
      <div class="content">
        <img src="static/images/more_video_vqa_cases.png">
        <h5>
          <p style="font-family:Times New Roman">More cases on our Open-VQA video benchmark.
          </p>
        </h5>
      </div>
    </div>

    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">

    <div class="column">
      <div class="content">
        <img src="static/images/more_owleval_cases1.png">
        <h5>
          <p style="font-family:Times New Roman">More cases on our OwlEval benchmark.
          </p>
        </h5>
      </div>
    </div>

    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">

    <div class="column">
      <div class="content">
        <img src="static/images/more_owleval_cases2.png">
        <h5>
          <p style="font-family:Times New Roman">More cases on our OwlEval benchmark.
          </p>
        </h5>
      </div>
    </div>

    </div>
  </div>
</section>

<section class="section" id="BibTeX">
<div class="container is-max-desktop content">
  <h2 class="title">BibTeX</h2>
  <pre><code>

@article{,
title={What Matters in Training a GPT4-Style Language Model with Multi-modal Inputs?},
author={},
journal={arXiv preprint arXiv:},
year={2023}
}
</code></pre>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="https://arxiv.org/abs/2211.10526">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/ranery/ranery.github.io/tree/master/castling-vit">source code</a>
              (adapted from <a href="https://github.com/nerfies/nerfies.github.io">nerfies</a>) of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>

</html>
